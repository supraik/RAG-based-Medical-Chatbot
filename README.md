# HaleAI Medical Chatbot

A production-ready RAG (Retrieval-Augmented Generation) chatbot for medical information powered by Google Gemini, Pinecone vector database, and a clean React frontend.

## ğŸš€ Quick Start

### Backend Setup

```bash
cd backend
python backend_api.py
```

Backend will start on http://localhost:8000

### Frontend Setup

```bash
cd frontend

# Option 1: Double-click start_server.bat (Windows)
# Option 2: PowerShell
./start_server.ps1

# Option 3: Manual
python -m http.server 5500
```

Frontend will open at http://localhost:5500/index.html

## ğŸ“‹ Project Structure

```
HaleAI/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ backend_api.py      # FastAPI REST API
â”‚   â”œâ”€â”€ chatbot.py          # Main chatbot logic
â”‚   â”œâ”€â”€ llm_handler.py      # Gemini/HuggingFace LLM
â”‚   â”œâ”€â”€ vector_store.py     # Pinecone vector DB
â”‚   â”œâ”€â”€ rag_processor.py    # RAG pipeline & reranking
â”‚   â”œâ”€â”€ data_processor.py   # PDF processing
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ helper.py       # Utility functions
â”‚   â”‚   â””â”€â”€ prompt.py       # System prompts
â”‚   â””â”€â”€ data/               # Medical PDF documents
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html          # React UI (self-contained)
â”‚   â”œâ”€â”€ start_server.bat    # Windows launcher
â”‚   â”œâ”€â”€ start_server.ps1    # PowerShell launcher
â”‚   â””â”€â”€ README.md           # Frontend documentation
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Prerequisites

- **Python 3.8+** (3.11 recommended)
- **Pinecone Account** (free tier: https://app.pinecone.io/)
- **Google AI Studio API Key** (free: https://aistudio.google.com/app/apikey)
- **HuggingFace Token** (optional, for fallback: https://huggingface.co/settings/tokens)

## ğŸ“¦ Installation

### 1. Create Virtual Environment

```bash
# Using conda (recommended)
conda create -n HaleAI python=3.11 -y
conda activate HaleAI

# OR using venv
python -m venv venv
# Windows: venv\Scripts\activate
# Linux/Mac: source venv/bin/activate
```

### 2. Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### 3. Configure API Keys

Create a `.env` file in the `backend/` folder:

```env
PINECONE_API_KEY=your_pinecone_api_key
GOOGLE_API_KEY=your_gemini_api_key
HF_TOKEN=your_huggingface_token  # Optional
```

### 4. Add Medical Documents

Place PDF files in `backend/data/` folder

### 5. Setup Vector Database (First Time Only)

```bash
cd backend
python setup.py
```

This will:

- Process your PDF documents
- Generate embeddings
- Upload to Pinecone vector store

## ğŸ¯ Features

### Backend (FastAPI)

- âœ… **Google Gemini Integration** (gemini-1.5-flash)
- âœ… **HuggingFace Fallback** (google/flan-t5-small)
- âœ… **Pinecone Vector Store** (384-dim embeddings)
- âœ… **Cross-Encoder Reranking** (ms-marco-MiniLM-L-6-v2)
- âœ… **Streaming Responses** (Server-Sent Events)
- âœ… **Session Management** (conversation history)
- âœ… **CORS Enabled** (all frontend ports)

### Frontend (HTML/React)

- âœ… **No Build Required** (CDN-based React)
- âœ… **Streaming Chat UI** (real-time token display)
- âœ… **Markdown Rendering** (bold, italic, code, lists)
- âœ… **Conversation History** (maintained across messages)
- âœ… **Analytics Dashboard** (accuracy, latency metrics)
- âœ… **Dark Mode** (toggle)
- âœ… **Export Conversations** (JSON/TXT)
- âœ… **Responsive Design** (mobile-friendly)

## ğŸ”Œ API Endpoints

| Endpoint                         | Method | Description          |
| -------------------------------- | ------ | -------------------- |
| `/api/health`                    | GET    | Health check         |
| `/api/chat`                      | POST   | Non-streaming chat   |
| `/api/chat/stream`               | POST   | Streaming chat (SSE) |
| `/api/analytics`                 | GET    | System analytics     |
| `/api/conversations/{id}/export` | GET    | Export conversation  |

## âš™ï¸ Configuration

Edit `backend/config.py` to customize:

```python
# LLM Configuration
GEMINI_MODEL = "gemini-1.5-flash"           # Gemini model
USE_HF_FALLBACK = True                       # Enable HF fallback
HF_FALLBACK_MODEL = "google/flan-t5-small"   # Fallback model

# Vector Store
PINECONE_INDEX_NAME = "medical-chatbot"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# RAG Settings
CHUNK_SIZE = 800
CHUNK_OVERLAP = 200
RETRIEVER_K = 8          # Documents to retrieve
RERANK_TOP_K = 3         # Documents after reranking
```

## ğŸ§ª Testing

```bash
cd backend

# Test vector store connection
python test_pinecone.py

# Test generic RAG pipeline
python test_generic_rag.py

# Test environment variables
python test/test_env.py
```

## ğŸ“– Usage

### Starting the Application

**Terminal 1 - Backend:**

```bash
cd backend
python backend_api.py
```

**Terminal 2 - Frontend:**

```bash
cd frontend
python -m http.server 5500
```

**Open Browser:**
http://localhost:5500/index.html

### Using the Chat

1. Type your medical question
2. Press Enter or click Send
3. Watch the response stream in real-time
4. Conversation context is maintained automatically

### Example Questions

- "What are the symptoms of diabetes?"
- "How is hypertension treated?"
- "What causes migraine headaches?"

## ğŸ› Troubleshooting

### Backend Issues

**"Pinecone connection failed"**

- Check `PINECONE_API_KEY` in `.env`
- Verify index name matches `config.py`

**"Gemini API error"**

- Check `GOOGLE_API_KEY` in `.env`
- Verify API quota (free tier limits)
- Enable HuggingFace fallback

**"ModuleNotFoundError"**

- Activate virtual environment
- Run `pip install -r requirements.txt`

### Frontend Issues

**"Failed to fetch"**

- Ensure backend is running on port 8000
- Check CORS configuration in `backend_api.py`

**Streaming not working**

- Check browser console for errors
- Verify `/api/chat/stream` endpoint accessibility

**Markdown not rendering**

- Clear browser cache
- Refresh page (Ctrl+F5)

## ğŸ’¡ Tips for Best Results

1. **First Query is Slow**: Model loading takes 20-60 seconds initially
2. **Be Specific**: Ask clear, focused medical questions
3. **Context Matters**: The chatbot maintains conversation history
4. **Check Sources**: Review the source documents provided with answers
5. **Rate Limits**: Free tier APIs have usage limits

## ğŸš¢ Deployment

### Backend (FastAPI)

```bash
# Production server
pip install uvicorn[standard]
uvicorn backend_api:app --host 0.0.0.0 --port 8000 --workers 4
```

### Frontend

Simply host `frontend/index.html` on any static file server:

- GitHub Pages
- Netlify
- Vercel
- AWS S3 + CloudFront

## ğŸ“Š System Requirements

**Minimum:**

- CPU: 2 cores
- RAM: 4 GB
- Disk: 2 GB free

**Recommended:**

- CPU: 4+ cores
- RAM: 8 GB
- Disk: 5 GB free

## ğŸ” Security Notes

- Never commit `.env` file to git
- Use environment variables for production
- Implement rate limiting for production deployment
- Add authentication for sensitive medical data

## ğŸ“ License

This project is for educational purposes. Consult with medical professionals for actual medical advice.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“ Support

For issues and questions:

1. Check the troubleshooting section
2. Review backend logs in `backend/logs/`
3. Check browser console for frontend errors

## ğŸ“ Learn More

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pinecone Documentation](https://docs.pinecone.io/)
- [Google Gemini API](https://ai.google.dev/docs)
- [RAG Explained](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

**Built with â¤ï¸ using FastAPI, React, Pinecone, and Google Gemini**
